{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Assignment 2\n",
        "\n",
        "âš ï¸ **Important! Submission Requirements**\n",
        "\n",
        "1. **Do NOT clear notebook logs or outputs.**  \n",
        "   Every output cell must remain visible (e.g., print logs, progress logs, summary messages).\n",
        "\n",
        "2. **The notebook must be *self-contained*.**  \n",
        "   - You should be able to run it **from top to bottom once** without any manual steps.  \n",
        "   - No need for any external scripts, config files, or multiple runs.\n",
        "   - Creating multiple cells/blocks is OK.\n",
        "\n",
        "3. **Single `.ipynb` implementation.**  \n",
        "   Do not create separate Python files or folders other than the generated image output directory.\n",
        "\n",
        "4. **Clean, readable, and well-commented code.**\n",
        "\n",
        "5. **Submission file name format:**  \n",
        "   â†’ `studentID_name.zip`  \n",
        "   (Example: `2025000000_jungbeomlee.zip`)\n",
        "   - This file should contain `COSE-474-02-Assignment_2.ipynb`.\n",
        "---\n"
      ],
      "metadata": {
        "id": "HbcdIbJrUwOL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. Build Your Own RNN (Forward Only)\n",
        "**ðŸŽ¯ Goal**\n",
        "\n",
        "\n",
        "In this problem, you will implement a simple (vanilla) RNN from scratch in PyTorch and verify that its forward pass matches torch.nn.RNN when they share the same parameters.\n",
        "\n",
        "You will not train this RNN. The goal is to:\n",
        "\n",
        "- Understand how the hidden state is updated over time.\n",
        "\n",
        "- Implement the forward computation manually.\n",
        "\n",
        "- Run a small sanity check to confirm that your implementation is correct.\n",
        "\n",
        "We will implement:\n",
        "\n",
        "1. MyRNNCell: computes a single RNN step $h_t$ from $(x_t, h_{t-1})$.\n",
        "\n",
        "2. MyRNN: applies MyRNNCell repeatedly over the time dimension.\n",
        "\n",
        "3. A sanity check: copy weights from nn.RNN to MyRNN and verify that the outputs match.\n",
        "\n",
        "Please fill in all TODO parts. Do not change function signatures."
      ],
      "metadata": {
        "id": "AmdrHdWBVZIc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# setup\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "device"
      ],
      "metadata": {
        "id": "u0b92waAUyMW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MyRNNCell(nn.Module):\n",
        "    \"\"\"\n",
        "    A single vanilla RNN cell:\n",
        "        h_t = tanh(Wx * x_t + Wh * h_{t-1} + b_h)\n",
        "    \"\"\"\n",
        "    def __init__(self, input_dim: int, hidden_dim: int):\n",
        "        super().__init__()\n",
        "        # define your weights, ignore output head (Wyh in our lecture note) for this assignment\n",
        "        self.Wx =  # bias: False\n",
        "        self.Wh =  # bias: True\n",
        "\n",
        "    def forward(self, x_t: torch.Tensor, h_prev: torch.Tensor) -> torch.Tensor:\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            x_t:   (batch_size, input_dim)\n",
        "            h_prev:(batch_size, hidden_dim)\n",
        "        Returns:\n",
        "            h_t:   (batch_size, hidden_dim)\n",
        "        \"\"\"\n",
        "\n",
        "        # implement your codes to compute h_t from x_t and h_prev\n",
        "\n",
        "\n",
        "        return h_t"
      ],
      "metadata": {
        "id": "Mf4yyh_0W3Fb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MyRNN(nn.Module):\n",
        "    \"\"\"\n",
        "    A simple RNN that applies MyRNNCell over a sequence.\n",
        "    We assume batch_first=True, i.e., input shape is (batch, seq_len, input_dim).\n",
        "    Initialize h_0 as a zero tensor\n",
        "    \"\"\"\n",
        "    def __init__(self, input_dim: int, hidden_dim: int):\n",
        "        super().__init__()\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.cell = MyRNNCell(input_dim, hidden_dim)\n",
        "\n",
        "    def forward(self, x: torch.Tensor, h0: torch.Tensor | None = None):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            x:  (batch_size, seq_len, input_dim)\n",
        "            h0: (batch_size, hidden_dim) initial hidden state, or None\n",
        "        Returns:\n",
        "            outputs: (batch_size, seq_len, hidden_dim)\n",
        "            h_T:     (batch_size, hidden_dim)  # hidden state at last time step\n",
        "        \"\"\"\n",
        "        batch_size, seq_len, _ = x.size()\n",
        "\n",
        "\n",
        "\n",
        "        outputs = []\n",
        "\n",
        "        # TODO: iterate over time steps and apply the RNN cell\n",
        "\n",
        "\n",
        "        return outputs, h_T"
      ],
      "metadata": {
        "id": "JXs9axd1XaLP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Sanity Check: Compare with torch.nn.RNN\n",
        "\n",
        "Here we:\n",
        "\n",
        "- Create a PyTorch nn.RNN and your MyRNN with the same dimensions.\n",
        "\n",
        "- Copy the parameters from nn.RNN into MyRNN so they represent the same RNN.\n",
        "\n",
        "- Feed the same input and compare outputs.\n",
        "\n",
        "If your implementation is correct, the mean absolute difference should be very small (e.g., < 1e-6)."
      ],
      "metadata": {
        "id": "7HReVqn6ZfIm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Hyperparameters for the sanity check\n",
        "batch_size = 2\n",
        "seq_len = 5\n",
        "input_dim = 4\n",
        "hidden_dim = 3\n",
        "\n",
        "# Create PyTorch RNN and your MyRNN\n",
        "torch_rnn = nn.RNN(input_dim, hidden_dim, batch_first=True, nonlinearity=\"tanh\")\n",
        "my_rnn    = MyRNN(input_dim, hidden_dim)\n",
        "\n",
        "# Move to device (optional, but good practice)\n",
        "torch_rnn = torch_rnn.to(device)\n",
        "my_rnn    = my_rnn.to(device)\n",
        "\n",
        "# 1) Copy weights from torch_rnn to my_rnn\n",
        "with torch.no_grad():\n",
        "    my_rnn.cell.Wx.weight.copy_(torch_rnn.weight_ih_l0)\n",
        "\n",
        "    # Copy hidden-to-hidden weights\n",
        "    my_rnn.cell.Wh.weight.copy_(torch_rnn.weight_hh_l0)\n",
        "\n",
        "    # Combine the two biases from torch_rnn into one bias in MyRNNCell\n",
        "    combined_bias = torch_rnn.bias_ih_l0 + torch_rnn.bias_hh_l0\n",
        "    my_rnn.cell.Wh.bias.copy_(combined_bias)\n",
        "\n",
        "# 2) Create a random input and initial hidden state\n",
        "x = torch.randn(batch_size, seq_len, input_dim, device=device)\n",
        "\n",
        "# torch.nn.RNN expects h0 of shape (num_layers, batch, hidden_dim)\n",
        "h0_torch = torch.randn(1, batch_size, hidden_dim, device=device)\n",
        "h0_my    = h0_torch.squeeze(0)  # (batch, hidden_dim)\n",
        "\n",
        "# 3) Forward pass\n",
        "out_torch, h_torch = torch_rnn(x, h0_torch)      # out_torch: (B, L, H), h_torch: (1, B, H)\n",
        "out_my, h_my       = my_rnn(x, h0_my)            # out_my:   (B, L, H), h_my:    (B, H)\n",
        "\n",
        "# 4) Compare outputs\n",
        "output_diff = torch.abs(out_torch - out_my).mean().item()\n",
        "hidden_diff = torch.abs(h_torch.squeeze(0) - h_my).mean().item()\n",
        "\n",
        "print(f\"Mean absolute difference (outputs): {output_diff:.8f}\")\n",
        "print(f\"Mean absolute difference (hidden):  {hidden_diff:.8f}\")\n",
        "\n",
        "assert output_diff < 1e-6 and hidden_diff < 1e-6, \"RNN implementation does not match torch.nn.RNN!\"\n",
        "print(\"âœ… Sanity check passed! Your MyRNN matches torch.nn.RNN (forward).\")"
      ],
      "metadata": {
        "id": "d70-rq7AYENE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. Word-level Language Modeling with LSTM\n",
        "\n",
        "In this problem, you will implement a simple word-level language model using an LSTM in PyTorch.\n",
        "You will:\n",
        "\n",
        "1. Load a text corpus from corpus.txt (Tiny Shakespeare).\n",
        "\n",
        "2. Tokenize the corpus into words and build a vocabulary.\n",
        "\n",
        "3. Create a dataset of inputâ€“target word sequences for next-word prediction.\n",
        "\n",
        "4. Implement an LSTM-based language model using nn.Embedding, nn.LSTM, and nn.Linear.\n",
        "\n",
        "5. Train the model to minimize cross-entropy loss using teacher-forcing.\n",
        "\n",
        "6. Implement a function to generate text given a prompt.\n",
        "\n",
        "For an input sequence of length $L$,\n",
        "- Input: $[w_1, w_2, ..., w_L]$\n",
        "- Output: $[w_2, w_3, ..., w_{L+1}]$\n",
        "\n",
        "\n",
        "Requirements:\n",
        "1. Training loss should be under 0.5.\n",
        "2. Try multiple inputs to \"generate_text\" function and see if your model generates feasible texts."
      ],
      "metadata": {
        "id": "eNShDsRQahe4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import io\n",
        "import os\n",
        "import math\n",
        "import random\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "device"
      ],
      "metadata": {
        "id": "Z4I-fRP2diGA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Download 'corpus.txt' and upload it to the current directory.\n",
        "\n",
        "with open(\"corpus.txt\", \"r\", encoding=\"utf-8\") as f:\n",
        "    text = f.read()\n",
        "\n",
        "print(\"Total characters:\", len(text))\n",
        "print(\"Sample text:\")\n",
        "print(text[:500])"
      ],
      "metadata": {
        "id": "ce5aXRmjd1eF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Simple word-level tokenization: split on whitespace.\n",
        "# (You may modify this if you want, but keep it simple.)\n",
        "\n",
        "tokens = text.strip().split()\n",
        "print(\"Number of tokens:\", len(tokens))\n",
        "print(\"First 50 tokens:\", tokens[:50])"
      ],
      "metadata": {
        "id": "kmEEN1rUd-KW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Build vocabulary: map each unique word to an integer id.\n",
        "vocab = sorted(set(tokens))\n",
        "word_to_idx = {word: i for i, word in enumerate(vocab)}\n",
        "idx_to_word = {i: word for word, i in word_to_idx.items()}\n",
        "\n",
        "vocab_size = len(vocab)\n",
        "print(\"Vocab size:\", vocab_size)"
      ],
      "metadata": {
        "id": "7n9BSIo-eFxs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "seq_len = 20\n",
        "batch_size = 10 # you may change this\n",
        "\n",
        "class WordLMDataset(Dataset):\n",
        "    def __init__(self, tokens, seq_len, word_to_idx):\n",
        "        self.seq_len = seq_len\n",
        "        self.word_to_idx = word_to_idx\n",
        "\n",
        "        # Convert entire corpus into indices once.\n",
        "        self.indices = [self.word_to_idx[w] for w in tokens]\n",
        "\n",
        "    def __len__(self):\n",
        "        # number of possible sliding windows\n",
        "\n",
        "        return len(self.indices) - self.seq_len - 1\n",
        "        # ============================\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "\n",
        "        # ====== YOUR CODE HERE ======\n",
        "        x = self.indices[<implement here>]\n",
        "        y = self.indices[<implement here>]\n",
        "        x = torch.tensor(x, dtype=torch.long)\n",
        "        y = torch.tensor(y, dtype=torch.long)\n",
        "        # ============================\n",
        "\n",
        "        return x, y\n",
        "\n",
        "dataset = WordLMDataset(tokens, seq_len, word_to_idx)\n",
        "print(len(dataset), dataset[0][0].shape, dataset[0][1].shape)\n",
        "loader = DataLoader(dataset, batch_size=batch_size, shuffle=True, drop_last=True)\n"
      ],
      "metadata": {
        "id": "7vguXIN0eR7F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the LSTM Language Model\n",
        "class WordLSTM(nn.Module):\n",
        "    def __init__(self, vocab_size, embed_dim=128, hidden_dim=256, num_layers=1):\n",
        "        super().__init__()\n",
        "        self.vocab_size = vocab_size\n",
        "        self.embed_dim = embed_dim\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.num_layers = num_layers\n",
        "\n",
        "        self.embedding = nn.Embedding(vocab_size, embed_dim)\n",
        "        self.lstm = nn.LSTM(\n",
        "            input_size=embed_dim,\n",
        "            hidden_size=hidden_dim,\n",
        "            num_layers=num_layers,\n",
        "            batch_first=True,\n",
        "        )\n",
        "        self.fc = nn.Linear(hidden_dim, vocab_size)\n",
        "\n",
        "    def forward(self, x, hidden=None):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            x:      (batch, seq_len) int64 word indices\n",
        "            hidden: (h0, c0) or None\n",
        "        Returns:\n",
        "            logits: (batch, seq_len, vocab_size)\n",
        "            hidden: (h_T, c_T)\n",
        "        \"\"\"\n",
        "\n",
        "        # ====== YOUR CODE HERE ======\n",
        "\n",
        "        # ============================\n",
        "\n",
        "        return logits, hidden"
      ],
      "metadata": {
        "id": "DOqpaGFJesma"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Training Loop\n",
        "\n",
        "embed_dim = 128\n",
        "hidden_dim = 256\n",
        "num_layers = 1\n",
        "num_epochs = 5   # you may increase/decrease\n",
        "learning_rate = 1e-3\n",
        "\n",
        "model = WordLSTM(vocab_size, embed_dim, hidden_dim, num_layers).to(device)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
        "\n",
        "def train_one_epoch(model, loader, optimizer, criterion, device):\n",
        "    model.train()\n",
        "    total_loss = 0.0\n",
        "\n",
        "    for x, y in loader:\n",
        "        x = x.to(device)   # (B, L)\n",
        "        y = y.to(device)   # (B, L)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # TODO: forward, compute loss, backward, step\n",
        "        # Remember:\n",
        "        #   logits: (B, L, V)\n",
        "        #   CrossEntropyLoss expects: (B*L, V) and targets (B*L,)\n",
        "\n",
        "        # ====== YOUR CODE HERE ======\n",
        "\n",
        "\n",
        "        # ============================\n",
        "\n",
        "        total_loss += loss.item()\n",
        "\n",
        "    return total_loss / len(loader)\n",
        "\n",
        "for epoch in range(1, num_epochs + 1):\n",
        "    train_loss = train_one_epoch(model, loader, optimizer, criterion, device)\n",
        "    print(f\"Epoch {epoch:02d} | train loss: {train_loss:.4f}\")"
      ],
      "metadata": {
        "id": "zLhmrM4VfC_w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_text(model, start_text, max_new_tokens=50, temperature=1.0):\n",
        "    \"\"\"\n",
        "    Args:\n",
        "        model:      trained WordLSTM\n",
        "        start_text: string, seed prompt (e.g., \"To be or not to be\")\n",
        "        max_new_tokens: how many words to generate\n",
        "        temperature: softmax temperature (>0). Higher = more random.\n",
        "    \"\"\"\n",
        "    model.eval()\n",
        "\n",
        "    # Tokenize start_text into words\n",
        "    words = start_text.strip().split()\n",
        "\n",
        "    # Map to indices (OOV words are mapped to 0 or some fallback)\n",
        "    # Here, we simply skip unknown words or map them to 0.\n",
        "    idxs = []\n",
        "    for w in words:\n",
        "        if w in word_to_idx:\n",
        "            idxs.append(word_to_idx[w])\n",
        "        else:\n",
        "            # unknown word â†’ you can choose to skip or map to 0\n",
        "            # here we map to 0:\n",
        "            idxs.append(0)\n",
        "\n",
        "    x = torch.tensor([idxs], dtype=torch.long, device=device)  # (1, L)\n",
        "    hidden = None\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for _ in range(max_new_tokens):\n",
        "            logits, hidden = model(x, hidden)\n",
        "            # Focus on the last time step\n",
        "            last_logits = logits[:, -1, :] / temperature   # (1, V)\n",
        "            probs = F.softmax(last_logits, dim=-1)         # (1, V)\n",
        "            next_idx = torch.multinomial(probs, num_samples=1).item()\n",
        "\n",
        "            words.append(idx_to_word[next_idx])\n",
        "\n",
        "            # Now feed only the last predicted token as next input\n",
        "            x = torch.tensor([[next_idx]], dtype=torch.long, device=device)\n",
        "\n",
        "    return \" \".join(words)"
      ],
      "metadata": {
        "id": "iiAZj7HaylZB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(generate_text(model, start_text=\"To be\", max_new_tokens=30, temperature=1.0))"
      ],
      "metadata": {
        "id": "A9p0GMrsytpn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. Character-level Language Modeling with LSTM\n",
        "\n",
        "In this problem, you will build **a character-level language model** using PyTorchâ€™s nn.LSTM.\n",
        "Unlike word-level language modeling, here we treat each character as a token.\n",
        "\n",
        "You will:\n",
        "\n",
        "- Build a character vocabulary from the corpus.\n",
        "\n",
        "- Implement a CharLMDataset that creates (input, target) sequences for next-character prediction.\n",
        "\n",
        "- Implement a CharLSTM model using nn.Embedding and nn.LSTM.\n",
        "\n",
        "- Train the model with teacher forcing: at each time step, the model receives the ground-truth previous characters as input.\n",
        "\n",
        "\n",
        "We will use the same corpus file as in Problem 2.\n",
        "\n",
        "\n",
        "Requirements:\n",
        "1. Training loss should be under 1.1.\n",
        "2. Try multiple inputs to \"generate_characters\" function and see if your model generates feasible texts."
      ],
      "metadata": {
        "id": "KkhJjuvGBIXN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "device"
      ],
      "metadata": {
        "id": "QHB_UilqBhfD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the full corpus as a single string\n",
        "with open(\"corpus.txt\", \"r\", encoding=\"utf-8\") as f:\n",
        "    text = f.read()\n",
        "\n",
        "print(\"Total number of characters in corpus:\", len(text))\n",
        "print(\"Preview:\")\n",
        "print(text[:500])"
      ],
      "metadata": {
        "id": "0XYPS5LyBkwR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Build character vocabulary\n",
        "chars = sorted(list(set(text)))\n",
        "char_to_idx = {c: i for i, c in enumerate(chars)}\n",
        "idx_to_char = {i: c for c, i in char_to_idx.items()}\n",
        "vocab_size_char = len(chars)"
      ],
      "metadata": {
        "id": "Zp6bLXDMBnuA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Character-level dataset\n",
        "\n",
        "seq_len_char = 100\n",
        "\n",
        "class CharLMDataset(Dataset):\n",
        "    def __init__(self, text: str, seq_len: int):\n",
        "        super().__init__()\n",
        "        self.seq_len = seq_len\n",
        "        # Convert entire text into a list of indices\n",
        "        self.indices = [char_to_idx[c] for c in text]\n",
        "\n",
        "    def __len__(self):\n",
        "\n",
        "        return len(self.indices) - self.seq_len\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "\n",
        "        # ====== YOUR CODE HERE ======\n",
        "        x = self.indices[<implement here>]\n",
        "        y = self.indices[<implement here>]\n",
        "        x = torch.tensor(x, dtype=torch.long)\n",
        "        y = torch.tensor(y, dtype=torch.long)\n",
        "        # ============================\n",
        "\n",
        "        return x, y\n",
        "\n",
        "dataset_char = CharLMDataset(text, seq_len_char)\n",
        "batch_size = 64\n",
        "\n",
        "loader_char = DataLoader(dataset_char, batch_size=batch_size, shuffle=True, drop_last=True)\n",
        "\n",
        "print(\"Number of training examples:\", len(dataset_char))\n",
        "x_example, y_example = next(iter(loader_char))\n",
        "print(\"Example batch shapes:\", x_example.shape, y_example.shape)  # (B, L), (B, L)"
      ],
      "metadata": {
        "id": "6OEciq2EBu8r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Character-level LSTM model\n",
        "\n",
        "class CharLSTM(nn.Module):\n",
        "    def __init__(self, vocab_size: int, embed_dim: int = 128, hidden_dim: int = 256, num_layers: int = 1):\n",
        "        super().__init__()\n",
        "        self.vocab_size = vocab_size\n",
        "        self.embed_dim = embed_dim\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.num_layers = num_layers\n",
        "\n",
        "        self.embedding = nn.Embedding(vocab_size, embed_dim)\n",
        "        self.lstm = nn.LSTM(embed_dim, hidden_dim, num_layers, batch_first=True)\n",
        "        self.fc = nn.Linear(hidden_dim, vocab_size)\n",
        "\n",
        "    def forward(self, x: torch.Tensor, hidden=None):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            x:      (batch_size, seq_len)  integer character indices\n",
        "            hidden: (h_0, c_0) or None\n",
        "        Returns:\n",
        "            logits: (batch_size, seq_len, vocab_size)\n",
        "            hidden: (h_T, c_T)\n",
        "        \"\"\"\n",
        "\n",
        "\n",
        "        # ====== YOUR CODE HERE ======\n",
        "\n",
        "        # ============================\n",
        "\n",
        "        return logits, hidden"
      ],
      "metadata": {
        "id": "Jl5RmsT8CSN6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# training\n",
        "\n",
        "model_char = CharLSTM(vocab_size_char, embed_dim=128, hidden_dim=256, num_layers=1).to(device)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer_char = torch.optim.Adam(model_char.parameters(), lr=1e-3)\n",
        "\n",
        "num_epochs = 5  # you may increase or decrease this\n",
        "\n",
        "\n",
        "def train_char_epoch(model, loader, optimizer, criterion, device):\n",
        "    model.train()\n",
        "    total_loss = 0.0\n",
        "\n",
        "    for x, y in loader:\n",
        "        x = x.to(device)   # (B, L)\n",
        "        y = y.to(device)   # (B, L)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # TODO: forward pass, compute loss, backprop, optimizer step\n",
        "\n",
        "\n",
        "        # ====== YOUR CODE HERE ======\n",
        "\n",
        "\n",
        "        # ============================\n",
        "\n",
        "        total_loss += loss.item()\n",
        "\n",
        "    return total_loss / len(loader)\n",
        "\n",
        "\n",
        "for epoch in range(1, num_epochs + 1):\n",
        "    train_loss = train_char_epoch(model_char, loader_char, optimizer_char, criterion, device)\n",
        "    print(f\"[Char LM] Epoch {epoch}: train loss = {train_loss:.4f}\")"
      ],
      "metadata": {
        "id": "pJYTycALCbxY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Text generation (sampling)\n",
        "\n",
        "def generate_characters(model, start_text: str, max_len: int = 300, temperature: float = 1.0) -> str:\n",
        "    \"\"\"\n",
        "    Generate text from a trained character-level language model.\n",
        "    \"\"\"\n",
        "    model.eval()\n",
        "    chars = list(start_text)\n",
        "\n",
        "    # Convert initial prompt to indices\n",
        "    idxs = [char_to_idx.get(c, 0) for c in chars]  # unknown chars -> 0\n",
        "    x = torch.tensor([idxs], dtype=torch.long, device=device)  # (1, L0)\n",
        "\n",
        "    hidden = None\n",
        "\n",
        "    with torch.no_grad():\n",
        "        # First, run the prompt through the model to update hidden state\n",
        "        if x.size(1) > 0:\n",
        "            logits, hidden = model(x, hidden)\n",
        "\n",
        "        # Now, generate new characters step by step\n",
        "        for _ in range(max_len):\n",
        "            # Use the last character as input\n",
        "            last_idx = idxs[-1]\n",
        "            x_last = torch.tensor([[last_idx]], dtype=torch.long, device=device)  # (1, 1)\n",
        "\n",
        "            logits, hidden = model(x_last, hidden)  # logits: (1, 1, V)\n",
        "            last_logits = logits[:, -1, :] / temperature  # (1, V)\n",
        "            probs = F.softmax(last_logits, dim=-1)        # (1, V)\n",
        "\n",
        "            # Sample next character index\n",
        "            next_idx = torch.multinomial(probs, num_samples=1).item()\n",
        "            idxs.append(next_idx)\n",
        "            chars.append(idx_to_char[next_idx])\n",
        "\n",
        "    return \"\".join(chars)\n"
      ],
      "metadata": {
        "id": "DX15sEt4CtM5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(generate_characters(model_char, start_text=\"KING \", max_len=300, temperature=0.8))\n"
      ],
      "metadata": {
        "id": "3hohroUKCwSG"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}